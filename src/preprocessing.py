"""Preprocessing utilities for YouTube + Instagram comment sentiment analysis."""

from __future__ import annotations

import json
import os
import re
from collections import Counter
from dataclasses import dataclass
from typing import Dict, Iterable, List, Optional, Tuple

try:
    import emoji
except ImportError:
    emoji = None

from config import PROCESSED_DATA_PATH, RAW_DATA_PATH

# Chemin du fichier Instagram
INSTAGRAM_DATA_PATH = "data/raw/inst.json"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# REGEX DE NETTOYAGE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_URL_RE = re.compile(r"https?://\S+|www\.\S+", re.IGNORECASE)
_MENTION_RE = re.compile(r"@\w+", re.IGNORECASE)
_HASHTAG_RE = re.compile(r"#\w+", re.IGNORECASE)
_TIME_RE = re.compile(r"\b\d{1,2}:\d{2}(?::\d{2})?\b")
_MULTISPACE_RE = re.compile(r"\s+")
_ARABIC_LETTERS_RE = re.compile(r"[\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF]")
_LATIN_LETTERS_RE = re.compile(r"[a-zA-Z]")
_DIGIT_RE = re.compile(r"\d")

_EMOJI_RE = re.compile(
    "["
    "\U0001F300-\U0001F5FF"
    "\U0001F600-\U0001F64F"
    "\U0001F680-\U0001F6FF"
    "\U0001F700-\U0001F77F"
    "\U0001F780-\U0001F7FF"
    "\U0001F800-\U0001F8FF"
    "\U0001F900-\U0001F9FF"
    "\U0001FA00-\U0001FA6F"
    "\U0001FA70-\U0001FAFF"
    "\U00002600-\U000026FF"
    "\U00002700-\U000027BF"
    "\U0000FE00-\U0000FE0F"
    "\U0000200D"
    "\U00000023\U000020E3"
    "\U0000002A\U000020E3"
    "\U00000030-\U00000039\U000020E3"
    "\U0001F1E0-\U0001F1FF"
    "]+",
    flags=re.UNICODE,
)

_PUNCTUATION_RE = re.compile(
    r"[!\"#$%&'()*+,\-./:;<=>?@\[\\\]^_`{|}~"
    r"\u060C\u061B\u061F\u0640\u066A\u066B\u066C\u066D"
    r"\u00AB\u00BB\u2018\u2019\u201C\u201D"
    r"\u2026\u2013\u2014"
    r"]+"
)


def _remove_all_emojis(text: str) -> str:
    if not text:
        return text
    if emoji is not None:
        return emoji.replace_emoji(text, replace=" ")
    return _EMOJI_RE.sub(" ", text)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PATTERNS MORPHOLOGIQUES DARIJA
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_DARIJA_VERB_PREFIX_RE = re.compile(
    r"^(ÙƒÙŠ|ÙƒØ§|ÙƒÙ†|ÙƒØª|ØºØ§|Ù…Ø§|ØªØ§|Ù†Ø§|ÙŠØ§)"
    r"[\u0600-\u06FF]{2,}"
)

_DARIJA_NEGATION_RE = re.compile(
    r"^Ù…Ø§[\u0600-\u06FF]{2,}(Ø´|Ø´ÙŠ)$"
)

_DARIJA_LATIN_NEGATION_RE = re.compile(
    r"^ma[a-z]{2,}(?:ch|sh)$",
    re.IGNORECASE
)

_DARIJA_LATIN_VERB_PREFIX_RE = re.compile(
    r"^(?:ka|ki|kan|kat|tan|tat|gha|ghan|na|ta|ya|ba|da)[a-z]{2,}$",
    re.IGNORECASE
)

_DARIJA_SUFFIX_RE = re.compile(
    r"[\u0600-\u06FF]{3,}(Ù†ÙŠ|ØªÙŠ|ØªÙˆ|Ù†Ø§Ù‡|Ù‡Ø§Ø´|Ù†ÙŠØ´|Ù„ÙŠÙ‡|Ù„ÙŠÙ‡Ø§|Ù„ÙŠÙ‡Ù…|ØªÙ‡Ù…|Ù†Ø§Ù‡Ø§|ÙˆÙ‡Ø§|ÙˆÙ‡|Ù‡Ù…|ÙƒÙ…|Ù†Ø§)$"
)

_DARIJA_CONTRACTION_RE = re.compile(
    r"^(Ø¯Ø§Ù„|Ø¯Ù„|ÙØ§Ù„|ÙÙ„|Ø¨Ø§Ù„|Ø¨Ù„|Ø¯ÙŠØ§Ù„|ÙÙŠÙ‡|Ø¨ÙŠÙ‡|Ø¹Ù„ÙŠÙ‡)[\u0600-\u06FF]*$"
)

_DARIJA_PLURAL_RE = re.compile(
    r"[\u0600-\u06FF]{3,}(ÙŠÙ†|Ø§Øª|ÙˆØ§|ÙŠÙˆ|ÙŠÙŠÙ†)$"
)

_DARIJA_ELONGATION_RE = re.compile(
    r"^[a-z]*([aeiou])\1{2,}[a-z]*$",
    re.IGNORECASE
)


def _has_darija_morphology(tokens: List[str]) -> int:
    count = 0
    for tok in tokens:
        tok_lower = tok.lower()
        if _DARIJA_VERB_PREFIX_RE.match(tok):
            count += 1
        elif _DARIJA_NEGATION_RE.match(tok):
            count += 1
        elif _DARIJA_SUFFIX_RE.match(tok):
            count += 1
        elif _DARIJA_CONTRACTION_RE.match(tok):
            count += 1
        elif _DARIJA_PLURAL_RE.match(tok):
            count += 1
        elif _DARIJA_LATIN_NEGATION_RE.match(tok_lower):
            count += 1
        elif _DARIJA_LATIN_VERB_PREFIX_RE.match(tok_lower):
            count += 1
        elif _DARIJA_ELONGATION_RE.match(tok_lower):
            count += 1
    return count


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# STOPWORDS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_STOPWORDS = frozenset({
    "Ùˆ", "ÙÙŠ", "Ø¹Ù„Ù‰", "Ù…Ù†", "Ø§Ù„Ù‰", "Ø¥Ù„Ù‰", "Ø¹Ù†", "Ù†Ø¹Ù…",
    "Ù‡Ùˆ", "Ù‡ÙŠ", "Ù‡Ù…", "Ù‡Ø§", "Ù‡Ø°Ø§", "Ù‡Ø°Ù‡", "Ø°Ù„Ùƒ", "ØªÙ„Ùƒ",
    "ÙˆØ§Ø´", "Ø´Ù†Ùˆ", "Ø§Ù†", "Ø£Ù†", "Ø¥Ù†",
    "a", "b", "c", "d", "de", "des", "du", "et", "le", "la", "les", 
    "un", "une", "est", "sont", "avec", "pour", "pas", "que", "qui",
    "the", "and", "to", "of", "for", "in", "is", "are", "it", "this",
    "that", "you", "i", "a", "an", "be", "have", "has", "was", "were",
    "Ù‡Ø§Ø¯", "Ù‡Ø§Ø¯ÙŠ", "Ù‡Ø§Ø¯Ùˆ", "Ø¯ÙŠØ§Ù„", "Ù„ÙŠ", "Ø§Ù„Ù„ÙŠ",
    "ÙƒØ§Ù†", "ÙŠÙƒÙˆÙ†", "ØºØ§Ø¯ÙŠ", "ÙƒØ§ÙŠÙ†", "Ø±Ø§Ù‡",
    "Ø¹Ù†Ø¯", "ÙƒÙ„", "Ø¨Ø­Ø§Ù„", "Ø£Ùˆ", "Ø§Ùˆ", "ÙŠØ¹Ù†ÙŠ", "ÙƒÙŠÙ",
    "Ø¨Ø§Ø´", "Ø­ØªÙ‰", "Ø¨Ù„ÙŠ", "Ø´ÙŠ", "Ø¯Ø§Ùƒ", "Ø¯ÙŠÙƒ", "Ù‡Ø¯Ø§Ùƒ",
    "ÙÙŠÙ‡", "ÙÙŠÙ‡Ø§", "Ù…Ø¹Ø§", "Ø¹Ù„ÙŠÙ‡", "Ø¹Ù„ÙŠÙ‡Ø§",
    "Ø¨ÙŠÙ‡", "Ø¨ÙŠÙ‡Ø§", "Ù„ÙŠÙ‡", "Ù„ÙŠÙ‡Ø§",
    "Ù…Ù†ÙŠÙ†", "ÙƒÙŠÙÙ…Ø§", "ÙØ§Ø´", "Ù…Ù„ÙŠ",
    "ØºÙŠ", "ØºÙŠØ±", "ØªØ§", "Ø­ØªØ§",
    "Ø±Ø§", "ÙŠØ§Ù„Ø§Ù‡", "Ø§ÙˆØ§", "Ø§ÙŠÙˆØ§",
    "Ø¢Ø´", "Ø´ÙƒÙˆÙ†", "ÙƒÙˆÙ†", "Ø§Ù„Ø§", "Ø¥Ù„Ø§",
})


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DARIJA MARKERS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_DARIJA_MARKERS = frozenset({
    "Ø¨Ø²Ø§Ù", "ÙˆØ§Ø´", "Ø¹Ø§ÙØ§Ùƒ", "Ø­ÙŠØª", "ÙƒØ§ÙŠÙ†", "ØºØ§Ø¯ÙŠ",
    "Ø®Ø§ÙŠØ¨", "Ø²ÙˆÙŠÙ†", "Ù…Ø²ÙŠØ§Ù†", "Ù…ÙƒØ§ÙŠÙ†", "Ø¨ØºÙŠØª", "Ø±Ø§Ùƒ",
    "Ø¯Ø§Ø¨Ø§", "Ø´ÙˆÙŠØ©", "Ù‡Ø§Ø¯", "Ø¯ÙŠØ§Ù„", "Ù…Ø³ÙƒÙŠÙ†", "ÙˆØ§Ø¹Ø±",
    "ÙƒÙ†Ø¯ÙŠØ±", "ÙƒÙ†Ø´ÙˆÙ", "ÙƒÙ†Ù‚ÙˆÙ„", "ÙƒÙŠÙ‚ÙˆÙ„", "ÙƒÙŠØ¯ÙŠØ±",
    "ÙƒÙ†Ù…ÙˆØª", "Ø¨ØºØ§", "ÙƒÙ„Ø´ÙŠ", "Ø¹Ù†Ø¯Ùˆ", "Ø¹Ù†Ø¯Ù‡Ø§",
    "Ù…Ø§ÙƒØ§ÙŠÙ†", "Ø®Ø§ØµÙƒ", "Ø®Ø§ØµÙ†ÙŠ", "Ø¨ØµØ­", "ØµØ§ÙÙŠ", "Ø²Ø¹Ù…Ø§",
    "Ø¹Ø§Ø¯", "ÙŠØ§Ù„Ø§Ù‡", "Ù…Ø§Ø´ÙŠ", "ÙƒÙŠÙØ§Ø´", "Ù‡Ø§Ø¯Ø´ÙŠ", "Ø±Ø§Ù‡",
    "ÙƒØ§Ø¹", "ÙˆØ§Ù„Ùˆ", "ÙŠÙ‚Ø¯Ø±", "Ø®ØµÙƒ", "Ø®ØµÙ†ÙŠ",
    "Ù…Ø§Ø¨ØºÙŠØªØ´", "ÙƒÙ†Ø¨ØºÙŠ", "Ø¹Ø¬Ø¨Ù†ÙŠ", "ÙƒØ±Ù‡Øª", "Ø¶Ø­ÙƒØª", "Ø¨ÙƒÙŠØª",
    "Ø®ÙØª", "ÙØ±Ø­Øª", "Ø²Ø¹ÙØª",
    "Ø¹Ø±ÙØªÙŠ", "Ø¹Ø±ÙØª", "Ø¹Ø±ÙÙ†Ø§", "Ø¹Ø±ÙÙˆ",
    "Ù†Ø¯ÙŠØ±", "Ù†Ø¯ÙŠØ±Ùˆ", "Ø¯ÙŠØ±", "Ø¯ÙŠØ±ÙŠ", "Ø¯Ø§Ø±Øª", "Ø¯Ø§Ø±",
    "Ø¨Ù‚Ø§Øª", "Ø¨Ù‚Ø§", "Ø¨Ù‚ÙŠØª", "Ø¨Ù‚ÙŠÙ†Ø§",
    "ØªÙ„Ø¹Ø¨Ùˆ", "ØªÙ„Ø¹Ø¨", "ÙƒÙŠÙ„Ø¹Ø¨", "ÙƒÙŠÙ„Ø¹Ø¨Ùˆ",
    "ÙƒÙŠØ®Ù„ÙŠÙ†ÙŠ", "ÙƒÙŠØ®Ù„ÙŠ", "Ø®Ù„Ø§Ù†ÙŠ", "Ø®Ù„Ø§Øª",
    "Ù†Ø´ÙˆÙ", "Ù†Ø´ÙˆÙÙˆ", "Ø´ÙØª", "Ø´ÙØªÙŠ", "Ø´Ø§ÙÙˆ",
    "Ù†Ù‚ÙˆÙ„", "Ù†Ù‚ÙˆÙ„Ùˆ", "Ù‚Ù„Øª", "Ù‚Ù„ØªÙŠ", "Ù‚Ø§Ù„Ùˆ",
    "Ù†Ù…Ø´ÙŠ", "Ù†Ù…Ø´ÙŠÙˆ", "Ù…Ø´ÙŠØª", "Ù…Ø´Ø§", "Ù…Ø´Ø§Øª",
    "Ù†ÙˆÙ‚Ù", "ÙˆÙ‚ÙØª", "ÙˆÙ‚Ù",
    "Ù†ÙƒØªØ¨", "ÙƒØªØ¨Øª", "ÙƒØªØ¨",
    "Ù†ÙÙ‡Ù…", "ÙÙ‡Ù…Øª", "ÙÙ‡Ù…ØªÙŠ", "ÙÙ‡Ù…Ù†Ø§",
    "Ù†Ø³Ù…Ø¹", "Ø³Ù…Ø¹Øª", "Ø³Ù…Ø¹ØªÙŠ",
    "Ù†Ø®Ø¯Ù…", "Ø®Ø¯Ù…Øª", "Ø®Ø¯Ø§Ù…", "Ø®Ø¯Ø§Ù…Ø©",
    "Ø¬Ø¨Øª", "Ø¬ÙŠØª", "Ø¬Ø§Ø¨", "Ø¬Ø§Ø¨Øª",
    "Ø¨Ø¯Ø§", "Ø¨Ø¯ÙŠØª", "Ø¨Ø¯ÙŠÙ†Ø§",
    "Ø¯Ø±Øª", "Ø¯Ø§Ø±Øª", "Ø¯Ø§Ø±Ùˆ", "Ø¯Ø±Ù†Ø§",
    "Ø·Ù„Ø¹", "Ø·Ù„Ø¹Øª", "Ø·Ù„Ø¹Ùˆ",
    "Ø¯ÙˆØ²", "Ø¯ÙˆØ²Øª", "Ø¯ÙˆØ²Ù‡Ø§",
    "Ù‡Ø¯ÙŠÙƒ", "Ù‡Ø¯Ø§", "Ù‡Ø¯ÙˆÙƒ", "Ù‡Ø§Ø¯ÙˆÙƒ", "Ù‡Ø§Ø¯Ùˆ",
    "Ø¯ÙŠØ§Ù„ÙŠ", "Ø¯ÙŠØ§Ù„Ùƒ", "Ø¯ÙŠØ§Ù„Ùˆ", "Ø¯ÙŠØ§Ù„Ù‡Ø§",
    "Ø¯ÙŠØ§Ù„Ù†Ø§", "Ø¯ÙŠØ§Ù„ÙƒÙ…", "Ø¯ÙŠØ§Ù„Ù‡Ù…",
    "ÙˆØµØ§ÙÙŠ", "Ø³Ø§ÙÙŠ", "ÙˆØ§Ø®Ø§", "Ø®Ù„Ø§Øµ",
    "Ø§Ù„Ø§Ù‡", "Ø¨ØµØ§Ø­", "ØµØ­ÙŠØ­",
    "ÙŠØ§Ù„Ù„Ù‡", "Ø§Ø¬ÙŠ", "Ø³ÙŠØ±", "Ø¬ÙŠ",
    "Ù…Ø§Ø¹Ø±ÙØªØ´", "Ù…Ø§ÙƒØ§Ù†Ø´", "Ù…Ø§ÙÙ‡Ù…ØªØ´",
    "Ù…Ø§Ø¹Ø¬Ø¨Ù†ÙŠØ´", "Ù…Ø§Ù‚Ø¯Ø±Ø´", "Ù…Ø§Ø¹Ù†Ø¯ÙŠØ´",
    "Ù…Ø§Ø¨Ù‚Ø§Ø´", "Ù…Ø§Ø¬Ø§Ø´", "Ù…Ø§ÙƒØ§ÙŠÙ†Ø§Ø´",
    "Ù…Ø³Ù…Ø¹ØªØ´", "Ù…Ø´ÙØªØ´", "Ù…Ø§Ø´ÙØªØ´",
    "Ù„Ø§ÙŠÙƒ", "ÙÙŠØ¯ÙŠÙˆ", "ÙˆØ§Ø¹Ø±Ø©",
    "Ø´Ù†Ùˆ", "Ø¹Ù„Ø§Ø´", "ÙÙŠÙ†", "ÙƒÙŠÙØ§Ø´",
    "Ø®ÙˆÙŠØ§", "Ø®ØªÙŠ", "ØµØ§Ø­Ø¨ÙŠ", "ØµØ§Ø­Ø¨ØªÙŠ",
    "ÙˆÙ„ÙŠØ¯Ø§Øª", "Ø¯Ø±Ø§Ø±ÙŠ", "Ø¨Ù†Ø§Øª", "ÙˆÙ„Ø§Ø¯",
    "ÙÙ„ÙˆØ³", "Ø®Ø¯Ù…Ø©", "Ù‚Ø±Ø§ÙŠØ©",
    "Ø²Ù†Ù‚Ø©", "Ø­ÙˆÙ…Ø©", "Ø¨Ù„Ø§Ø¯", "Ø¨Ù„Ø§Ø¯Ù†Ø§",
    "Ù…ØºØ±Ø¨", "Ù…ØºØ±Ø¨ÙŠ", "Ù…ØºØ±Ø¨ÙŠØ©",
    "Ù†ØªØ§", "Ù†ØªÙŠ", "Ø­Ù†Ø§", "Ù‡ÙˆÙ…Ø§", "Ù†ØªÙˆÙ…Ø§",
    "ØªØ¨Ø§Ø±Ùƒ", "ØªØ¨Ø§Ø±Ùƒ Ø§Ù„Ù„Ù‡", "Ø§Ù„Ù„Ù‡", "Ù„Ù„Ù‡",
    "Ø§Ù„ÙØ±Ø¯Ø©", "Ù„ÙØ±Ø¯Ø©", "Ù„ÙˆØ§Ù„", "Ù„ÙˆÙ„",
    "Ø§Ù„Ù…Ø­Ø³Ø§Ø¯Ø©", "Ù„Ù…Ø­Ø³Ø§Ø¯Ø©", "Ù…Ø­Ø³Ø§Ø¯Ø©",
    "Ø§Ù„Ù…Ù‡Ø¯Ø§ÙˆÙŠ", "Ù„Ù…Ù‡Ø¯Ø§ÙˆÙŠ", "Ù…Ù‡Ø¯Ø§ÙˆÙŠ",
    "Ø³Ø®ÙˆÙ†ÙŠØ§Øª", "Ø³Ø®ÙˆÙ†ÙŠØ©",
    "ÙØ±Ø´Ù‡Ø§", "ÙØ±Ø§Ø´",
    "Ù…Ø³Ù…Ù†Ø©", "Ù…Ø³Ù…Ù†",
    "Ø¯Ø§ÙŠØ±Ø©", "Ø¯Ø§ÙŠØ±",
    "Ø®Ø§ÙˆÙŠ", "Ø®Ø§ÙˆÙŠØ©",
    "Ù…ÙŠÙƒØ±ÙˆØ¨",
    "Ø¯Ø¨Ø²", "Ø¯Ø§Ø¨Ø²",
    "ÙƒÙŠØªØ¹Ø§ÙŠØ±Ùˆ",  # Nouveau mot d'Instagram
    "Ø§Ù„Ø¯Ø¹Ø§ÙˆÙŠ", "Ø§Ù„Ø²ÙˆÙŠÙ†ÙŠÙ†",  # Nouveau
    # Arabizi
    "nt", "nta", "nti", "ntoma", "hna", "homa",
    "bghit", "bghiti", "bghina", "bghitk", "bghitkom", "bghitkoum",
    "ghadi", "ghada", "ghanmchi", "ghanndir",
    "bzaaf", "bzaf", "bzzaf",
    "7it", "hit",
    "3lach", "3la", "3lih", "3liha",
    "wach", "wash", "wesh",
    "chno", "chnou", "chnu",
    "fin", "feen", "fayn",
    "kifach", "kifash",
    "daba", "dab", "db",
    "bgh", "bghi", "bghina", "brina",
    "jib", "jibi", "jibha", "jibhom",
    "ahsan", "a7san", "7san",
    "b4ina", "brina", "bghina",
    "had", "hada", "hadi", "hadchi",
    "bda", "bdit", "bdina", "bdaw",
    "ash", "ach",
    "ken", "kan", "kant", "kano",
    "zouiin", "zwin", "zwiin", "zouiiin",
    "jit", "jiti", "jaw", "jina",
    "bakri", "bekri", "bakrii", "bakriii",
    "akhiran", "akhiiran", "akhiiiiran",
    "hchouma", "7chouma", "hchoumaa",
    "banliya", "banlia", "banlya",
    "deratni", "dartni", "dertni",
    "ferassi", "frassi", "frasi",
    "fiha",
    "khir", "kheir", "5ir",
    "malha", "mal7a",
    "darbo", "darbou", "darboh",
    "msamna", "msemna", "msmnna",
    "tlah", "tla7",
    "dayr", "dayer", "dayra", "dayrin",
    "lferda", "ferda", "lfirda",
    "lowl", "lwel", "louwel", "lowal",
    "dwzha", "dwzhaa", "dwezha", "dwzhaaa",
    "chkon", "chkoun",
    "skhoniyat", "skhouniyat", "s5oniyat",
    "dima", "diima", "diiima",
    "maghrib", "lmaghrib", "lmghrib", "mghrb",
    "lagitou", "lgito", "lgitoh",
    "lmehssada", "lm7ssada", "lmhssada", "mehssada",
    "lmehdaoui", "mehdaoui", "mehdawi", "lmhdawi",
    "zmatkoum", "zmatkom", "zmatkum",
    "frechha", "frachha",
    "gydato", "gidato", "guidato", "gidatou",
    "dabz", "dabez", "dabzz", "dabzza",
    "microb", "mikrob", "mkrob",
    "khawi", "khawya", "5awi",
    "kandir", "katdir", "kaydir",
    "kanchof", "katchof", "kaychof",
    "kan3rf", "kat3rf", "kay3rf",
    "kanbghi", "katbghi", "kaybghi",
    "kanmchi", "katmchi", "kaymchi",
    "mabghitch", "mafhmtch", "ma3rftch",
    "makynch", "makaynch",
    "mchitch",
    "mskine", "mskina", "mskin",
    "wlad", "wld", "wlidi",
    "kolchi", "kolshi", "kulshi",
    "mzyan", "mzyana", "mezyan", "meziana",
    "zwina", "zwin", "zween",
    "khoya", "khouya", "kho",
    "sahbi", "sa7bi", "s7abi",
    "dyal", "dial",
    "fhad",
    "bach", "bash",
    "bla", "bila",
    "ghi", "ghir", "gher",
    "ta", "taa",
    "rah",
    "sir", "siri",
    "aji",
    "safi", "saafi", "safii",
    "wa3r", "wa3ra", "waer", "waera",
    "khayb", "khayba", "5ayb",
    "walakin", "walakine",
    "machi", "mashi",
    "saraha", "sara7a", "sraha",
    "hamda", "hamdola", "hamdolah",
    "hamdoulah", "hamdoullah", "hamdoula", "hamdoulillah",
    "tbarkllah", "tbark", "tbarek",
    "machallah", "mashallah",
    "nchalah", "inchallah", "nshallah",
    "chhal", "ch7al",
    "chkoun", "shkoun",
    "3jbni", "ajebni", "3ajbni",
    "dir", "diri", "diro",
    "chouf", "chof",
    "smiya", "smiti",
    "zwaj",
    "khdma", "khedma",
    "flous", "flouss",
    "drari",
    "walo", "walou", "walloo",
    "wakha", "wakhha",
    "yak", "yakk", "yakkk",
    "ewa", "awa", "awaa", "ewaa",
    "ila", "ilaa", "ilaaa",
    "7ta", "hta", "7tta", "httaa",
    "bessa7", "bsa7", "bessa77",
    "3afak", "3afaak", "3afakk",
    "chokran", "choukran", "chokraan",
    "smeh", "sme7", "smehli", "sme7li",
    "blati", "blaati",
    "baraka", "barakaa", "barakaaa",
    "yalah", "yallah", "yallaah",
    "hania", "haniaa",
    "3lash", "3lachh",
    "finek", "finekk",
    "winek", "winekk",
    "chnahiya", "chnahia", "chnahiyaa",
    "labas", "labaas", "labass",
    "bikhir", "bi5ir",
    "hamdullah", "hamdollah", "7amdollah",
    "nchofo", "nchofou", "nchofouk",
    "tji", "tjii",
    "nmchi", "nmchii",
    "nrj3", "nrje3",
    "khouya", "khoya",
    "khti", "khtii",
    "wldi", "wldii",
    "bnti", "bntii",
    "rajli", "rajlii",
    "mrati", "mratii",
})


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GENERIC PHRASES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_GENERIC_PHRASES = frozenset({
    "Ù…Ø§ Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡", "Ù…Ø§Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡",
    "Ø§Ù„Ù„Ù‡Ù… Ø¨Ø§Ø±Ùƒ", "Ø§Ù„Ù„Ù‡Ù… Ø¨Ø§Ø±Ùƒ ÙÙŠÙƒ",
    "Ø§Ù„Ù„Ù‡ ÙŠØ¨Ø§Ø±Ùƒ", "Ø§Ù„Ù„Ù‡ ÙŠØ¨Ø§Ø±Ùƒ ÙÙŠÙƒ",
    "Ø¨Ø§Ø±Ùƒ Ø§Ù„Ù„Ù‡ ÙÙŠÙƒ", "Ø¨Ø§Ø±Ùƒ Ø§Ù„Ù„Ù‡",
    "Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡", "Ø³Ø¨Ø­Ø§Ù† Ø§Ù„Ù„Ù‡",
    "Ù„Ø§ Ø§Ù„Ù‡ Ø§Ù„Ø§ Ø§Ù„Ù„Ù‡", "Ù„Ø§ Ø¥Ù„Ù‡ Ø¥Ù„Ø§ Ø§Ù„Ù„Ù‡",
    "Ø§Ù„Ù„Ù‡Ù… ØµÙ„ Ø¹Ù„Ù‰ Ù…Ø­Ù…Ø¯", "Ø§Ù„Ù„Ù‡Ù… ØµÙ„ ÙˆØ³Ù„Ù…",
    "Ù…Ø§ Ø´Ø§Ø¡ Ø§Ù„Ù„Ù‡ ØªØ¨Ø§Ø±Ùƒ Ø§Ù„Ø±Ø­Ù…Ù†",
    "Ø³Ø¨Ø­Ø§Ù† Ø§Ù„Ù„Ù‡ ÙˆØ¨Ø­Ù…Ø¯Ù‡", "Ø³Ø¨Ø­Ø§Ù† Ø§Ù„Ù„Ù‡ Ø§Ù„Ø¹Ø¸ÙŠÙ…",
    "Ù„Ø§ Ø­ÙˆÙ„ ÙˆÙ„Ø§ Ù‚ÙˆØ© Ø§Ù„Ø§ Ø¨Ø§Ù„Ù„Ù‡",
    "Ù„Ø§ Ø­ÙˆÙ„ ÙˆÙ„Ø§ Ù‚ÙˆØ© Ø¥Ù„Ø§ Ø¨Ø§Ù„Ù„Ù‡",
    "Ø§Ø³ØªØºÙØ± Ø§Ù„Ù„Ù‡ Ø§Ù„Ø¹Ø¸ÙŠÙ…", "Ø§Ø³ØªØºÙØ± Ø§Ù„Ù„Ù‡",
    "Ø§Ù„Ù„Ù‡ Ø§ÙƒØ¨Ø±", "Ø§Ù„Ù„Ù‡ Ø£ÙƒØ¨Ø±",
})

_GENERIC_TOKENS = frozenset({
    "Ø§Ù„Ù„Ù‡Ù…", "Ø³Ø¨Ø­Ø§Ù†", "Ø§Ø³ØªØºÙØ±", "Ø§Ù„Ø­Ù…Ø¯",
})


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SENTIMENT LEXICON
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_SENTIMENT_LEXICON = frozenset({
    "Ø®Ø§ÙŠØ¨", "Ø®Ø§ÙŠØ¨Ø©", "Ø­Ø§Ù…Ø¶", "ÙŠØ®", "Ù…Ù‚Ø±Ù", "ÙƒØ§Ø±Ø«Ø©", "ÙƒØ§Ø±Ø«Ù‡", 
    "ÙØ§Ø´Ù„", "ÙØ§Ø´Ù„Ø©", "ØªÙÙˆ", "Ù‚Ø¨ÙŠØ­", "Ù‚Ø¨ÙŠØ­Ø©", "Ù†Ù‚Ø²", 
    "Ø­Ø´ÙˆÙ…Ø©", "Ø­Ø´ÙˆÙ…Ù‡", "Ø¹ÙŠØ¨", "Ø­Ù…Ø§Ø±", "Ø­Ù…Ø§Ø±Ø©", "Ø¨ØºÙ„",
    "Ø²Ø¨Ù„", "Ù‚Ù…Ø§Ù…Ø©", "Ù‚Ù…Ø§Ù…Ù‡", "Ø®Ù†Ø²", "ÙƒØ±ÙŠÙ‡", "ÙƒØ±ÙŠÙ‡Ø©",
    "Ù…ØºØ¨Ù†", "Ø³Ø§Ø®Ø·", "Ø¶Ø¹ÙŠÙ", "Ø¶Ø¹ÙŠÙØ©", "ØµÙØ±",
    "Ù†Ø§Ø²Ù„", "Ù†Ø§Ø²Ù„Ø©", "Ø®Ø§Ø³Ø±", "Ø®Ø§Ø³Ø±Ø©", "ÙØ§Ø³Ø¯", "ÙØ§Ø³Ø¯Ø©",
    "Ù…Ø±ÙŠØ¶", "Ù…Ø±ÙŠØ¶Ø©", "ØºØ¨ÙŠ", "ØºØ¨ÙŠØ©", "Ø¨Ù„ÙŠØ¯", "Ø¨Ù„ÙŠØ¯Ø©", "Ø¬Ø§Ù‡Ù„", "Ø¬Ø§Ù‡Ù„Ø©",
    "Ù…ÙƒØ±ÙˆÙ‡", "Ù…Ù†Ø§ÙÙ‚", "ÙƒØ°Ø§Ø¨", "ÙƒØ°Ø§Ø¨Ø©", "Ø­Ù‚ÙŠØ±", "Ø­Ù‚ÙŠØ±Ø©",
    "ÙˆØ³Ø®", "ÙˆØ³Ø®Ø©", "Ù‚Ø°Ø±", "Ù‚Ø°Ø±Ø©", "Ù†Ø¬Ø³",
    "Ù…ÙŠÙƒØ±ÙˆØ¨", "Ø®Ø§ÙˆÙŠ", "Ø®Ø§ÙˆÙŠØ©", "Ø¯Ø§Ø¨Ø²",
    "tfo", "9bi7", "fashel", "fashla",
    "hchouma", "7chouma",
    "5ayb", "khayb", "khayba",
    "7mar", "7mara", "hmar", "hmara",
    "m9rf", "mqrf",
    "microb", "mikrob",
    "khawi", "khawya",
    "dabz", "dabez",
    "Ø²ÙˆÙŠÙ†", "Ø²ÙˆÙŠÙ†Ø©", "Ø²ÙˆÙŠÙŠÙ†", "Ø²ÙˆÙŠÙŠÙ†Ø©",
    "Ù…Ø²ÙŠØ§Ù†", "Ù…Ø²ÙŠØ§Ù†Ø©", "Ù…Ø²ÙŠØ§Ù†ÙŠÙ†",
    "Ø±ÙˆØ¹Ø©", "Ø±Ø§Ø¦Ø¹", "Ø±Ø§Ø¦Ø¹Ø©",
    "ÙˆØ§Ø¹Ø±", "ÙˆØ§Ø¹Ø±Ø©", "ÙˆØ§Ø¹Ø±ÙŠÙ†",
    "Ø·ÙˆØ¨", "Ù†Ø¶ÙŠÙ", "Ù†Ø¶ÙŠÙØ©",
    "Ø¨ÙˆÙ…Ø¨Ø§", "Ù…Ù…ØªØ§Ø²", "Ù…Ù…ØªØ§Ø²Ø©",
    "Ø¹Ø¸ÙŠÙ…", "Ø¹Ø¸ÙŠÙ…Ø©", "Ø¬Ù…ÙŠÙ„", "Ø¬Ù…ÙŠÙ„Ø©",
    "Ø­Ù„Ùˆ", "Ø­Ù„ÙˆØ©", "Ø®Ø·ÙŠØ±", "Ø®Ø·ÙŠØ±Ø©",
    "ÙØ§Ø¨ÙˆØ±", "Ù‚Ù†Ø¨Ù„Ø©", "Ù‚Ù†Ø¨Ù„Ù‡",
    "Ø¯ÙŠÙ…Ø§", "ÙØ®Ø±", "ÙØ®ÙˆØ±",
    "Ø´ÙƒØ±Ø§", "Ø´ÙƒØ±Ø§Ù‹", "ØªØ­ÙŠØ©", "ØªØ­ÙŠÙ‡",
    "Ù†Ø¬Ù…", "Ù†Ø¬Ù…Ø©", "Ø¨Ø·Ù„", "Ø¨Ø·Ù„Ø©",
    "Ø§Ø³Ø·ÙˆØ±Ø©", "Ø§Ø³Ø·ÙˆØ±Ù‡", "Ø£Ø³Ø·ÙˆØ±Ø©",
    "Ø­Ø¨", "Ø­Ø¨ÙŠØª", "Ø¹Ø´Ù‚", "Ø¹Ø´Ù‚Øª",
    "ÙØ±Ø­Ø§Ù†", "ÙØ±Ø­Ø§Ù†Ø©", "Ø³Ø¹ÙŠØ¯", "Ø³Ø¹ÙŠØ¯Ø©",
    "Ù…Ø¨Ø±ÙˆÙƒ", "Ù…Ø¨Ø±ÙˆÙƒØ©", "ØªÙ‡Ù†Ø¦Ø©",
    "Ø£Ø­Ø³Ù†", "Ø§Ø­Ø³Ù†", "Ø®ÙŠØ±",
    "top", "tooop", "toooop",
    "bravo", "bravooo",
    "3jbni", "3ajbni", "ajebni",
    "zwin", "zwina", "zween", "zweena",
    "zouiin", "zouiiin",
    "wa3r", "wa3ra", "waer", "waera",
    "mzyan", "mzyana", "mezyan",
    "tbarkllah", "tbark", "tbarek",
    "machallah", "mashallah",
    "nice", "cool", "great", "amazing", "awesome",
    "love", "loved", "best",
    "ahsan", "a7san",
    "khir", "kheir", "5ir",
})


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MSA MARKERS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_MSA_MARKERS = frozenset({
    "ÙŠØ¬Ø¨", "ÙŠÙ†Ø¨ØºÙŠ", "Ù„ÙƒÙ†", "Ù„Ø°Ù„Ùƒ", "Ø¨Ø³Ø¨Ø¨",
    "Ø§Ù„Ø°ÙŠ", "Ø§Ù„ØªÙŠ", "Ø§Ù„Ø°ÙŠÙ†", "Ø§Ù„Ù„Ø§ØªÙŠ", "Ø§Ù„Ù„ÙˆØ§ØªÙŠ",
    "Ø­ÙŠØ«", "Ø¨Ø§Ù„ØªØ§Ù„ÙŠ", "ÙƒÙ…Ø§", "ÙˆÙ‚Ø¯", "ØªÙ…", "Ù„Ø¯Ù‰",
    "Ø£ÙŠØ¶Ø§Ù‹", "Ø£ÙŠØ¶Ø§", "Ø§ÙŠØ¶Ø§", "Ø¹Ù„Ø§ÙˆØ©", "ÙØ¶Ù„Ø§Ù‹", "ÙØ¶Ù„Ø§",
    "Ø¨ÙŠÙ†Ù…Ø§", "Ø±ØºÙ…", "Ù…Ù…Ø§", "Ø¥Ø°", "ÙˆÙÙ‚", "ÙˆÙÙ‚Ø§Ù‹",
    "Ù†Ø­Ùˆ", "Ø®Ù„Ø§Ù„", "Ø¶Ù…Ù†", "ØªØ¬Ø§Ù‡", "Ø¨Ø´Ø£Ù†",
    "Ù†Ø¸Ø±Ø§Ù‹", "Ù†Ø¸Ø±Ø§", "Ø¥Ø«Ø±", "Ø§Ø«Ø±",
    "Ø¹Ù‚Ø¨", "Ø³ÙˆÙ‰", "Ø¯ÙˆÙ†", "Ù‚Ø¨Ù„", "Ø¨Ø¹Ø¯", "Ø¹Ø¨Ø±", "Ø¶Ø¯",
    "ÙŠØªÙˆØ¬Ø¨", "ÙŠØ³ØªÙ„Ø²Ù…", "ÙŠØªØ·Ù„Ø¨", "ÙŠÙ‚ØªØ¶ÙŠ",
    "ÙŠÙØ¹Ø¯", "ÙŠØ¹Ø¯", "ØªÙØ¹Ø¯", "ØªØ¹Ø¯",
    "ÙŠÙØ¹ØªØ¨Ø±", "ÙŠØ¹ØªØ¨Ø±", "ØªÙØ¹ØªØ¨Ø±", "ØªØ¹ØªØ¨Ø±",
    "Ø£ÙƒØ¯", "Ø£ÙƒØ¯Øª", "ÙŠØ¤ÙƒØ¯", "ØªØ¤ÙƒØ¯",
    "Ø£Ø´Ø§Ø±", "Ø£Ø´Ø§Ø±Øª", "ÙŠØ´ÙŠØ±", "ØªØ´ÙŠØ±",
    "Ø£ÙˆØ¶Ø­", "Ø£ÙˆØ¶Ø­Øª", "ÙŠÙˆØ¶Ø­", "ØªÙˆØ¶Ø­",
    "ØµØ±Ø­", "ØµØ±Ø­Øª", "ÙŠØµØ±Ø­", "ØªØµØ±Ø­",
    "Ø§Ù„Ù…ÙˆØ§Ø·Ù†ÙˆÙ†", "Ø§Ù„Ù…ÙˆØ§Ø·Ù†ÙŠÙ†", "Ø§Ù„Ø­ÙƒÙˆÙ…Ø©", "Ø§Ù„Ø¯ÙˆÙ„Ø©",
    "Ø§Ù„Ù…Ø¬ØªÙ…Ø¹", "Ø§Ù„Ø³ÙŠØ§Ø³Ø©", "Ø§Ù„Ø§Ù‚ØªØµØ§Ø¯", "Ø§Ù„ØªÙ†Ù…ÙŠØ©",
    "Ø§Ù„Ù…Ø¤Ø³Ø³Ø§Øª", "Ø§Ù„Ø¥Ø¯Ø§Ø±Ø©", "Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†", "Ø§Ù„Ø¯Ø³ØªÙˆØ±",
})


_RELIGIOUS_MARKERS_STRICT = frozenset({
    "Ø§Ù„Ù„Ù‡Ù…", "Ø±Ø³ÙˆÙ„", "Ø§Ù„Ù†Ø¨ÙŠ", "Ø§Ù„Ù…Ø³Ù„Ù…ÙŠÙ†", "Ø§Ù„Ø¯Ø¹Ø§Ø¡",
    "ØµØ­Ø¨Ù‡", "Ø£Ø¬Ù…Ø¹ÙŠÙ†", "ÙˆØ³Ù„Ù…", "Ù†Ø¨ÙŠÙ†Ø§", "Ø¢Ù„Ù‡",
    "Ø§Ø³ØªØºÙØ±", "ÙˆØ¨Ø­Ù…Ø¯Ù‡",
    "ØµÙ„Ù‰ Ø§Ù„Ù„Ù‡ Ø¹Ù„ÙŠÙ‡ ÙˆØ³Ù„Ù…", "Ø¹Ù„ÙŠÙ‡ Ø§Ù„Ø³Ù„Ø§Ù…",
    "Ø±Ø¶ÙŠ Ø§Ù„Ù„Ù‡ Ø¹Ù†Ù‡", "Ø±Ø¶ÙŠ Ø§Ù„Ù„Ù‡ Ø¹Ù†Ù‡Ø§",
})


_SPAM_PATTERNS = frozenset({
    "subscribe", "channel", "please", "follow", "check",
    "click", "link", "bio", "visit", "website",
    "free", "win", "gift", "money", "earn",
    "giveaway", "promotion", "discount", "offer",
    "bonjour", "merci", "salut", "comment", "pourquoi",
    "parce", "vraiment", "tellement", "jamais", "toujours",
    "aujourd", "demain", "hier",
})


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class PreprocessConfig:
    min_tokens: int = 1
    darija_ratio_threshold: float = 0.4
    min_darija_hits: int = 1
    allow_mixed_script: bool = True
    allow_arabizi: bool = True
    allow_arabic_no_markers: bool = True
    arabic_no_markers_max_tokens: int = 18
    allow_latin_no_markers: bool = False
    latin_no_markers_max_tokens: int = 5
    keep_short_if_sentiment: bool = True
    drop_emoji_only: bool = True
    drop_religious_msa: bool = True
    drop_generic_phrases: bool = True
    drop_spam: bool = True


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FONCTIONS DE NETTOYAGE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _normalize_arabic(text: str) -> str:
    text = re.sub(r"[\u064B-\u065F\u0670]", "", text)
    text = text.replace("\u0640", "")
    text = text.replace("Ø£", "Ø§").replace("Ø¥", "Ø§").replace("Ø¢", "Ø§").replace("Ù±", "Ø§")
    text = text.replace("Ù‰", "ÙŠ").replace("Ø¤", "Ùˆ").replace("Ø¦", "ÙŠ")
    text = text.replace("Ø©", "Ù‡")
    return text


def _normalize_elongation(text: str) -> str:
    text = re.sub(r"([\u0600-\u06FF])\1{2,}", r"\1\1", text)
    text = re.sub(r"([a-zA-Z])\1{2,}", r"\1\1", text)
    return text


def _strip_noise(text: str) -> str:
    text = _URL_RE.sub(" ", text)
    text = _MENTION_RE.sub(" ", text)
    text = _HASHTAG_RE.sub(" ", text)
    text = _TIME_RE.sub(" ", text)
    text = _remove_all_emojis(text)
    return text


def _remove_punctuation(text: str) -> str:
    return _PUNCTUATION_RE.sub(" ", text)


def _basic_clean(text: str) -> str:
    text = text.replace("\u200f", " ").replace("\u200e", " ")
    text = text.replace("\u200b", "").replace("\u200c", "").replace("\u200d", "")
    text = _strip_noise(text)
    text = _normalize_arabic(text)
    text = _normalize_elongation(text)
    text = text.lower()
    text = _remove_punctuation(text)
    text = re.sub(r"[^a-z0-9\s\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF]", " ", text)
    text = _MULTISPACE_RE.sub(" ", text).strip()
    return text


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TOKENISATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _tokenize(text: str) -> List[str]:
    if not text:
        return []
    tokens = text.split()
    filtered: List[str] = []
    for tok in tokens:
        if not tok:
            continue
        if _DIGIT_RE.search(tok):
            if _ARABIC_LETTERS_RE.search(tok) or _LATIN_LETTERS_RE.search(tok):
                filtered.append(tok)
            continue
        if _ARABIC_LETTERS_RE.search(tok) or _LATIN_LETTERS_RE.search(tok):
            filtered.append(tok)
    return filtered


def _remove_stopwords(tokens: Iterable[str]) -> List[str]:
    return [tok for tok in tokens if tok not in _STOPWORDS]


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FONCTIONS DE DÃ‰TECTION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _has_letters(text: str) -> bool:
    return bool(_ARABIC_LETTERS_RE.search(text) or _LATIN_LETTERS_RE.search(text))


def _is_emoji_only(text: str) -> bool:
    if not text:
        return True
    cleaned = _strip_noise(text)
    return not _has_letters(cleaned)


def _darija_stats(tokens: List[str]) -> Dict[str, object]:
    if not tokens:
        return {
            "darija_hits": 0, "darija_morphology": 0, "msa_hits": 0,
            "spam_hits": 0, "has_arabic": False, "has_latin": False,
            "has_arabizi_digits": False,
        }
    darija_hits = 0
    msa_hits = 0
    spam_hits = 0
    for t in tokens:
        t_lower = t.lower()
        if t in _DARIJA_MARKERS or t_lower in _DARIJA_MARKERS:
            darija_hits += 1
        if t in _MSA_MARKERS:
            msa_hits += 1
        if t_lower in _SPAM_PATTERNS:
            spam_hits += 1
    darija_morphology = _has_darija_morphology(tokens)
    has_arabic = any(_ARABIC_LETTERS_RE.search(t) for t in tokens)
    has_latin = any(_LATIN_LETTERS_RE.search(t) for t in tokens)
    has_arabizi_digits = any(re.search(r"[23579]", t) for t in tokens)
    return {
        "darija_hits": darija_hits, "darija_morphology": darija_morphology,
        "msa_hits": msa_hits, "spam_hits": spam_hits,
        "has_arabic": has_arabic, "has_latin": has_latin,
        "has_arabizi_digits": has_arabizi_digits,
    }


def _is_darija(tokens: List[str], cfg: PreprocessConfig) -> bool:
    if not tokens:
        return False
    stats = _darija_stats(tokens)
    darija_hits = stats["darija_hits"]
    darija_morphology = stats["darija_morphology"]
    msa_hits = stats["msa_hits"]
    spam_hits = stats["spam_hits"]
    total_darija = darija_hits + darija_morphology
    
    if total_darija > 0:
        if msa_hits > 0:
            total = total_darija + msa_hits
            ratio = total_darija / total
            if ratio < cfg.darija_ratio_threshold:
                return False
        return True
    if msa_hits > 0 and total_darija == 0:
        return False
    if stats["has_arabizi_digits"]:
        return True
    if stats["has_arabic"] and stats["has_latin"]:
        return True
    if stats["has_arabic"] and not stats["has_latin"]:
        if len(tokens) > cfg.arabic_no_markers_max_tokens:
            return False
        return True
    if stats["has_latin"] and not stats["has_arabic"]:
        if cfg.drop_spam and spam_hits > 0:
            spam_ratio = spam_hits / len(tokens)
            if spam_ratio > 0.3:
                return False
        for tok in tokens:
            tok_lower = tok.lower()
            if tok_lower in _DARIJA_MARKERS:
                return True
        for tok in tokens:
            tok_lower = tok.lower()
            if _DARIJA_LATIN_NEGATION_RE.match(tok_lower):
                return True
            if _DARIJA_LATIN_VERB_PREFIX_RE.match(tok_lower):
                return True
        if cfg.allow_latin_no_markers and len(tokens) <= cfg.latin_no_markers_max_tokens:
            return True
        return False
    return False


def _has_short_sentiment(tokens: List[str]) -> bool:
    for tok in tokens:
        tok_lower = tok.lower()
        if tok in _SENTIMENT_LEXICON or tok_lower in _SENTIMENT_LEXICON:
            return True
    return False


def _is_generic_phrase(cleaned: str, tokens: List[str]) -> bool:
    if cleaned in _GENERIC_PHRASES:
        return True
    if not tokens:
        return True
    if len(tokens) <= 4:
        generic_count = sum(1 for tok in tokens if tok in _GENERIC_TOKENS)
        if generic_count == len(tokens):
            return True
    return False


def _is_religious_msa(tokens: List[str], cleaned: str) -> bool:
    if cleaned in _GENERIC_PHRASES:
        return True
    if not tokens:
        return True
    non_stop_tokens = [t for t in tokens if t not in _STOPWORDS]
    if not non_stop_tokens:
        return True
    religious_count = sum(1 for t in non_stop_tokens if t in _RELIGIOUS_MARKERS_STRICT)
    if len(non_stop_tokens) > 0:
        religious_ratio = religious_count / len(non_stop_tokens)
        return religious_ratio > 0.5
    return True


def _is_spam(tokens: List[str], cfg: PreprocessConfig) -> bool:
    if not cfg.drop_spam:
        return False
    if not tokens:
        return False
    spam_count = sum(1 for t in tokens if t.lower() in _SPAM_PATTERNS)
    if len(tokens) > 0:
        spam_ratio = spam_count / len(tokens)
        return spam_ratio > 0.5
    return False


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PIPELINE PRINCIPALE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _preprocess_comment_internal(
    text: str, cfg: PreprocessConfig
) -> Tuple[Optional[Dict[str, object]], Optional[str]]:
    if not isinstance(text, str):
        return None, "not_text"
    text = text.strip()
    if not text:
        return None, "empty"
    if cfg.drop_emoji_only and _is_emoji_only(text):
        return None, "emoji_only"
    cleaned = _basic_clean(text)
    if not cleaned or not _has_letters(cleaned):
        return None, "empty_after_clean"
    full_tokens = _tokenize(cleaned)
    if not full_tokens:
        return None, "empty_after_tokenize"
    if _is_spam(full_tokens, cfg):
        return None, "spam"
    is_darija_text = _is_darija(full_tokens, cfg)
    if not is_darija_text:
        if cfg.drop_religious_msa and _is_religious_msa(full_tokens, cleaned):
            return None, "religious_msa"
        return None, "not_darija"
    if cfg.drop_generic_phrases and _is_generic_phrase(cleaned, full_tokens):
        return None, "generic_phrase"
    tokens = _remove_stopwords(full_tokens)
    if not tokens:
        return None, "only_stopwords"
    if len(tokens) < cfg.min_tokens:
        if cfg.keep_short_if_sentiment and _has_short_sentiment(tokens):
            pass
        else:
            return None, "too_short"
    stats = _darija_stats(full_tokens)
    lang_hint = "darija"
    if stats["has_arabic"] and stats["has_latin"]:
        lang_hint = "mixed"
    elif stats["has_arabizi_digits"] and not stats["has_arabic"]:
        lang_hint = "arabizi"
    elif not stats["has_arabic"] and stats["has_latin"]:
        lang_hint = "arabizi"
    return {
        "text_clean": cleaned,
        "tokens": tokens,
        "lang_hint": lang_hint,
    }, None


def preprocess_comment(text: str, cfg: Optional[PreprocessConfig] = None) -> Optional[Dict[str, object]]:
    if cfg is None:
        cfg = PreprocessConfig()
    result, _ = _preprocess_comment_internal(text, cfg)
    return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CHARGEMENT / SAUVEGARDE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _load_json(path: str) -> List[Dict[str, object]]:
    with open(path, "r", encoding="utf-8") as handle:
        return json.load(handle)


def _save_json(path: str, data: List[Dict[str, object]]) -> None:
    with open(path, "w", encoding="utf-8") as handle:
        json.dump(data, handle, ensure_ascii=False, indent=2)


def _load_instagram_data(path: str) -> List[Dict[str, object]]:
    """Charge et convertit les donnÃ©es Instagram au format standard."""
    if not os.path.exists(path):
        print(f"  â„¹ï¸ Fichier Instagram non trouvÃ©: {path}")
        return []
    
    try:
        with open(path, "r", encoding="utf-8") as handle:
            instagram_data = json.load(handle)
        
        converted = []
        for item in instagram_data:
            text = item.get("text", "")
            if not text or len(text.strip()) < 2:
                continue
            
            item_id = item.get("id", "")
            comment_id = f"ig_{item_id}" if not str(item_id).startswith("ig_") else item_id
            
            post_url = item.get("postUrl", "")
            shortcode = ""
            if "/p/" in post_url:
                shortcode = post_url.split("/p/")[1].split("/")[0]
            
            converted.append({
                "comment_id": comment_id,
                "text": text,
                "video_id": shortcode or post_url,
                "author": item.get("ownerUsername", "unknown"),
                "date": item.get("timestamp", ""),
                "likes": item.get("likesCount", 0),
                "channel": f"@{item.get('ownerUsername', 'instagram')}",
                "source": "instagram",
            })
            
            for reply in item.get("replies", []):
                reply_text = reply.get("text", "")
                if not reply_text or len(reply_text.strip()) < 2:
                    continue
                reply_id = reply.get("id", "")
                converted.append({
                    "comment_id": f"ig_{reply_id}",
                    "text": reply_text,
                    "video_id": shortcode or post_url,
                    "author": reply.get("ownerUsername", "unknown"),
                    "date": reply.get("timestamp", ""),
                    "likes": reply.get("likesCount", 0),
                    "channel": f"@{reply.get('ownerUsername', 'instagram')}",
                    "source": "instagram",
                })
        
        print(f"  âœ… {len(converted)} commentaires Instagram chargÃ©s")
        return converted
        
    except Exception as e:
        print(f"  âš ï¸ Erreur chargement Instagram: {e}")
        return []


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# EXÃ‰CUTION PRINCIPALE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def run_preprocessing(
    raw_path: str = RAW_DATA_PATH,
    instagram_path: str = "data/raw/inst.json",
    output_path: str = PROCESSED_DATA_PATH,
    dropped_path: str = "data/processed/comments_dropped.json",
    cfg: Optional[PreprocessConfig] = None,
) -> Dict[str, int]:
    """ExÃ©cute le pipeline de prÃ©traitement (YouTube + Instagram)."""

    if cfg is None:
        cfg = PreprocessConfig()

    print("\nğŸ“‚ Chargement des donnÃ©es...")
    print(f"  ğŸ“º YouTube: {raw_path}")
    
    raw_items = []
    if os.path.exists(raw_path):
        raw_items = _load_json(raw_path)
        print(f"  âœ… {len(raw_items)} commentaires YouTube chargÃ©s")
    else:
        print(f"  âš ï¸ Fichier YouTube non trouvÃ©: {raw_path}")
    
    for item in raw_items:
        if "source" not in item:
            item["source"] = "youtube"
    
    print(f"  ğŸ“¸ Instagram: {instagram_path}")
    instagram_items = _load_instagram_data(instagram_path)
    
    all_items = raw_items + instagram_items
    
    seen_ids = set()
    unique_items = []
    for item in all_items:
        cid = item.get("comment_id")
        if cid and cid not in seen_ids:
            unique_items.append(item)
            seen_ids.add(cid)
    all_items = unique_items
    
    youtube_count = sum(1 for i in all_items if i.get("source") == "youtube")
    instagram_count = sum(1 for i in all_items if i.get("source") == "instagram")
    
    print(f"\nğŸ“Š Total avant traitement:")
    print(f"  ğŸ“º YouTube  : {youtube_count}")
    print(f"  ğŸ“¸ Instagram: {instagram_count}")
    print(f"  ğŸ“Š Total    : {len(all_items)}")
    
    print("\nğŸ”„ PrÃ©traitement en cours...")
    
    processed: List[Dict[str, object]] = []
    dropped: List[Dict[str, object]] = []
    counters: Counter = Counter()

    for item in all_items:
        text = item.get("text", "")
        result, reason = _preprocess_comment_internal(text, cfg)

        if result is None:
            counters["dropped"] += 1
            if reason:
                counters[f"dropped_{reason}"] += 1
            dropped.append({
                "comment_id": item.get("comment_id"),
                "text": text,
                "video_id": item.get("video_id"),
                "author": item.get("author"),
                "date": item.get("date"),
                "likes": item.get("likes"),
                "channel": item.get("channel"),
                "source": item.get("source", "unknown"),
                "drop_reason": reason,
            })
            continue

        enriched = {
            "comment_id": item.get("comment_id"),
            "text": text,
            "text_clean": result["text_clean"],
            "tokens": result["tokens"],
            "date": item.get("date"),
            "likes": item.get("likes"),
            "channel": item.get("channel"),
            "lang_hint": result["lang_hint"],
        }
        processed.append(enriched)
        counters["kept"] += 1

    unique_map: Dict[str, Dict[str, object]] = {}
    for item in processed:
        text_clean = item.get("text_clean")
        if isinstance(text_clean, str) and text_clean not in unique_map:
            unique_map[text_clean] = item
        else:
            counters["duplicates_removed"] += 1
    
    processed = list(unique_map.values())
    counters["final_count"] = len(processed)

    final_youtube = sum(1 for i in processed if i.get("source") == "youtube")
    final_instagram = sum(1 for i in processed if i.get("source") == "instagram")
    
    counters["final_youtube"] = final_youtube
    counters["final_instagram"] = final_instagram

    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    os.makedirs(os.path.dirname(dropped_path), exist_ok=True)
    
    _save_json(output_path, processed)
    _save_json(dropped_path, dropped)

    counters["total"] = len(all_items)
    return dict(counters)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# POINT D'ENTRÃ‰E
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    stats = run_preprocessing()
    
    print("\n" + "=" * 60)
    print(" RÃ‰SULTATS DU PRÃ‰TRAITEMENT")
    print("=" * 60)
    
    print(f"\n{'ENTRÃ‰E':<40}")
    print(f"  {'Total commentaires bruts':<35} : {stats.get('total', 0):>8}")
    
    print(f"\n{' SORTIE':<40}")
    print(f"  {'GardÃ©s (avant dÃ©dup)':<35} : {stats.get('kept', 0):>8}")
    print(f"  {'Doublons supprimÃ©s':<35} : {stats.get('duplicates_removed', 0):>8}")
    print(f"  {'Commentaires finaux':<35} : {stats.get('final_count', 0):>8}")
    
    print(f"\n{' PAR SOURCE':<40}")
    print(f"  {'ğŸ“º YouTube':<35} : {stats.get('final_youtube', 0):>8}")
    print(f"  {'ğŸ“¸ Instagram':<35} : {stats.get('final_instagram', 0):>8}")
    
    print(f"\n{'ğŸ—‘ï¸ SUPPRIMÃ‰S':<40}")
    print(f"  {'Total supprimÃ©s':<35} : {stats.get('dropped', 0):>8}")
    
    print(f"\n{'ğŸ“‹ DÃ©tail des suppressions:':<40}")
    drop_reasons = {k: v for k, v in stats.items() if k.startswith("dropped_")}
    for key in sorted(drop_reasons.keys(), key=lambda x: -drop_reasons[x]):
        reason = key.replace("dropped_", "")
        print(f"    {reason:<32} : {drop_reasons[key]:>6}")
    
    print("\n" + "=" * 60)